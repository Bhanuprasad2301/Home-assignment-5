# Home-assignment-5
Name: BHANU PRASAD
ID:700762758
1) The first code implements the core idea behind the Transformer’s scaled dot-product attention mechanism using NumPy. This mechanism allows every word or vector in a sequence to look at all other vectors and decide how important they are. The code starts with a softmax function that turns raw similarity scores into smooth probability distributions, ensuring the model gives appropriate levels of attention to different positions. Inside the attention function, the Query (Q) vectors are compared with Key (K) vectors to measure how related or relevant each pair of tokens is. These comparisons generate attention scores, which are then scaled to keep the values stable and prevent extremely large gradients. After scaling, softmax is applied to convert scores into normalized attention weights. These weights indicate which words deserve more focus in the final representation. The Value (V) vectors are then combined using these weights to produce context vectors, meaning each token receives information from other tokens it “attended” to. Finally, the code prints the attention weights and context outputs, allowing us to see how information is redistributed across the sequence. This shows how a Transformer layer builds richer understanding by mixing information across all tokens.
   
3) The second code is essentially a cleaner, simplified version of the first implementation, focusing clearly on the mathematical steps of attention without any extra distractions. It repeats the same attention mechanism but in a more structured way, making it easier to understand and relate directly to the theory described in research papers. The softmax function again converts raw scores into probabilities, allowing each word to decide how much it should focus on others. The scaled dot-product attention function calculates similarity between Query and Key vectors, scales these values to maintain numerical stability, and then uses softmax to transform them into meaningful attention weights. These weights determine the influence each token has on the final output. The Value vectors are then aggregated based on these weights to create context vectors, which represent updated and context-aware embeddings for each position. By running the script, the model generates random Q, K, and V inputs and shows how attention redistributes information across the sequence. This version serves as a simple theoretical demonstration of how attention works internally in modern deep learning models such as Transformers, focusing strictly on the logical flow of the algorithm.
